# -*- coding: utf-8 -*-
"""Simple Hash-Pre Image Attack (FFN).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vWxVKxrPBx7bJO2c_abuD_nAR9NOYyae

# Simple Hash

simple modulo hash function:
$$
    f(x) = x\mod c\text{, where }x \geq 0\text{, and }c = 2^{32} - 1.
$$
"""

import math
import struct

c = 2**32 - 1

class Hash_value:
    def __init__(self, hashvalue):
        self.bits = format(hashvalue,'b').zfill(32)
        self.integer =  hashvalue

def simple_digest(m):
  if isinstance(m, str) and all(c in '01' for c in m):  # If input is a bit string
    m = int(m,2)
  elif not isinstance(m, int):
    raise ValueError("Input must be a bit string or integer")
  return Hash_value((m + 8) % c)


print(simple_digest(123456789).bits)
print(simple_digest(123456789).integer)

"""# Generate Dataset"""

from google.colab import drive
drive.mount('/content/drive')

# Path to Google Drive
file_path = '/content/drive/MyDrive/Datasets/'

"""## Dataset for FFN
- Input: hash as normed integer
- Output: message as bitvector
"""

import numpy as np
import random
import struct

def H(m):
    return simple_digest(m)

def generate_bitstring(length):
    return format(random.randint(0,c - 1),'b').zfill(104)

def generate_random_bitstrings(num_samples, bitlength):
    bitstrings = set()
    while(len(bitstrings) < num_samples):
        bitstring = generate_bitstring(bitlength)
        bitstrings.add(bitstring)
    return bitstrings

def generate_dataset(num_samples=100000, msglength = 104):# 104 bit messages are processed in one block
    X = []  # Input (normalized Hashvalues)
    Y_int = []
    Y = []  # Output (128-Bit-Bitvectors)
    msgs = generate_random_bitstrings(num_samples, msglength)
    for msg in msgs:
        hash = H(msg)  # calculate 32-Bit-Hash
        hash_normalized = hash.integer / (2**32 - 1)  # Normalized to [0,1]
        msg_bits = np.array(list(msg), dtype=np.uint8)  # 128 Bit

        X.append([hash_normalized])
        Y.append(msg_bits)
        Y_int.append([int(msg,2)/c])

    X = np.array(X, dtype=np.float32)
    Y = np.array(Y, dtype=np.float32)
    Y_int = np.array(Y_int, dtype=np.float32)

    np.save(f"{file_path}X_FFN_simpleHash.npy", X)
    np.save(f"{file_path}Y_FFN_simpleHash.npy", Y)
    np.save(f"{file_path}Y_int_FFN_simpleHash.npy", Y_int)


generate_dataset(100000)

"""## Dataset for CNN
- Features: hash as bitvector
- Label: message as bitvector
"""

import numpy as np
import random
import struct

# MD5 Light, returning integer
def H(m) -> str:
    return simple_digest(m).bits

def generate_bitstring(length):
    return ''.join(random.choice('01') for _ in range(length))

def generate_random_bitstrings(num_samples, bitlength):
    bitstrings = set()
    while(len(bitstrings) < num_samples):
        bitstring = generate_bitstring(bitlength)
        bitstrings.add(bitstring)
    return bitstrings

def generate_dataset(num_samples=100000, msglength = 104):# 104 bit messages are processed in one block
    X = []  # Input (32-Bit-Vectors)
    Y = []  # Output (128-Bit-Bitvectors)
    msgs = generate_random_bitstrings(num_samples, msglength)
    for msg in msgs:
        hash = H(msg)  # calculate 32-Bit-Hash
        hash_bits = np.array(list(hash), dtype=np.uint8)  # 32 Bit
        msg_bits = np.array(list(msg), dtype=np.uint8)  # 128 Bit

        X.append(hash_bits)
        Y.append(msg_bits)

    X = np.array(X, dtype=np.float32)
    Y = np.array(Y, dtype=np.float32)

    np.save(f"{file_path}X_CNN_simpleHash.npy", X)
    np.save(f"{file_path}Y_CNN_simpleHash.npy", Y)


generate_dataset(100000)

!pip install scikeras scikit-optimize

"""# Feedforward Neural Network Based Pre Image Attack on MD5 Light

"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras.regularizers import l2

# load Dataset
X = np.load(f"{file_path}X_FFN_simpleHash.npy")  # Normalisierte Hashwerte
Y = np.load(f"{file_path}Y_FFN_simpleHash.npy")  # 104-Bit-Nachrichten als Bitvektoren
Y_int = np.load(f"{file_path}Y_int_FFN_simpleHash.npy")  # 104-Bit-Nachrichten als Integer

# Überprüfen der Datenform
print(f"X Shape: {X.shape}")  # (100000, 1)
print(f"Y Shape: {Y.shape}")  # (100000, 104)
print(f"Y_int Shape: {Y_int.shape}")  # (100000, 104)

# 80% Training, 20% Test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y_int, test_size=0.2, random_state=42)

"""#### Model Architectur:
Since the function is periodic and simple, a small fully connected neural network with ReLU activation should work.
"""

# Modellaufbau
def create_model(learning_rate=0.005, neurons=[256,512,256], activation_fct='relu', dropout_rate=0.2):
  model = Sequential([
    Input(shape=(1,)), # Input Layer
    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),
    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),  # Hidden layer
    #Dense(104, activation="sigmoid")  # 128 Neuronen, Sigmoid für bitweise Ausgabe
    Dense(1)  # Output layer
  ])

  model.compile(optimizer=Adam(learning_rate=learning_rate), loss="mse", metrics=["accuracy"])
  return model

model = create_model()
# Modellübersicht
model.summary()

# Modell trainieren
history = model.fit(X_train, Y_train, epochs=20, batch_size=64, validation_split=0.1)

"""## Bayesian Optimization"""

import optuna
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping

# Daten laden
X = np.load(f"{file_path}X_FFN_MD5light.npy")
Y = np.load(f"{file_path}Y_FFN_MD5light.npy")

# Train-Test-Split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

historys = []
params_ = []

# Ziel-Funktion für Optuna
def objective(trial):
    # Optimierbare Hyperparameter
    num_layers = trial.suggest_int("num_layers", 1, 5)
    neurons = trial.suggest_int("neurons", 128, 1024, step=128)
    learning_rate = trial.suggest_loguniform("learning_rate", 1e-5, 1e-2)
    batch_size = trial.suggest_int("batch_size", 32, 1024, step=32)
    activation = trial.suggest_categorical("activation", ["relu", "leaky_relu"])

    # Modell aufbauen
    model = Sequential()
    model.add(Input(shape=(1,)))
    model.add(Dense(neurons, activation=activation))

    for _ in range(num_layers - 1):
        model.add(Dense(neurons, activation=activation))

    model.add(Dense(104, activation="sigmoid"))  # Bitvektor als Ausgabe

    # Optimizer
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss="binary_crossentropy", metrics=["accuracy"])

    # EarlyStopping Callback erstellen
    early_stopping = EarlyStopping(
      monitor='val_loss',  # Überwacht die Validierungs-Loss
      patience=10,          # Stoppt, wenn sich die Loss für 5 Epochen nicht verbessert
      restore_best_weights=True  # Stellt die besten Gewichte wieder her
    )
    history = model.fit(X_train, Y_train, epochs=20, batch_size=batch_size, validation_split=0.1, verbose=0, callbacks = [early_stopping])
    historys.append(history)
    params_.append([num_layers,neurons,learning_rate,batch_size,activation])
    # Bewertung auf Testset
    loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)
    print(f"Loss: {loss}; Accuracy: {accuracy}")

    return 1 - loss  # Wir minimieren loss

# Bayesian Optimization starten
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=30)

# Beste Hyperparameter ausgeben
print("Beste Hyperparameter:", study.best_params)

"""# Installation"""

!pip install optuna